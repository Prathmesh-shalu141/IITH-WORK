# -*- coding: utf-8 -*-
"""foml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RT1XF0vPCcs3eKBEZU7UEm9kRGRCn6kg
"""

import argparse
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score, classification_report
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Define the function to make predictions and save them
def make_predictions(test_fname, predictions_fname, features, preprocessor, best_model):
    # Load test data
    test = pd.read_csv(test_fname)
    test_X = test[features].copy()

    # Apply preprocessing to test features
    test_X = preprocessor.transform(test_X)

    # Make predictions
    preds = best_model.predict(test_X)

    # Map the numeric predictions back to categorical labels
    pred_labels = ['low', 'medium', 'high']
    preds = [pred_labels[pred] for pred in preds]

    # Prepare the submission dataframe
    test_uid = test[["UID"]].copy()
    test_uid["Target"] = preds

    # Save the predictions to the output file
    test_uid.to_csv(predictions_fname, index=False)
    print(f"Predictions saved to {predictions_fname}")

# Main function to train the model
def main(train_file, test_file, predictions_file):
    print("Train File:", train_file)
    print("Test File:", test_file)
    print("Predictions File:", predictions_file)

    # Load the data
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)

    # Drop columns with more than 80% missing values
    high_missing_cols = [col for col in train_df.columns if train_df[col].isnull().mean() > 0.8]
    train_df.drop(columns=high_missing_cols, inplace=True)
    test_df.drop(columns=high_missing_cols, inplace=True)

    # Split into features and target
    global features
    features = train_df.drop(['UID', 'Target'], axis=1).columns.tolist()
    X_train = train_df[features]
    y_train = train_df['Target'].map({"low": 0, "medium": 1, "high": 2})

    # Identify Numerical and Categorical Columns
    numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()

    # Combined preprocessing pipeline
    global preprocessor
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ]), numerical_features),
            ('cat', Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))
            ]), categorical_features)
        ],
        n_jobs=-1
    )

    # Train-test split
    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
    X_train_preprocessed = preprocessor.fit_transform(X_train)
    X_valid_preprocessed = preprocessor.transform(X_valid)

    # Model: RandomForest Classifier
    rf_model = RandomForestClassifier(
        random_state=42,
        n_jobs=-1,
        class_weight='balanced_subsample',
        verbose=2
    )

    # Define an expanded hyperparameter grid for RandomizedSearchCV
    param_grid = {
        'n_estimators': [300, 500, 800, 1000],
        'max_depth': [15, 25, 35, 45],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 5],
        'max_features': ['auto', 'sqrt'],
        'bootstrap': [True, False],
    }

    # Hyperparameter tuning using RandomizedSearchCV
    random_search = RandomizedSearchCV(
        rf_model, param_distributions=param_grid, n_iter=15, cv=StratifiedKFold(5), verbose=2, random_state=42, n_jobs=-1, scoring='f1_macro'
    )
    random_search.fit(X_train_preprocessed, y_train)
    global best_model
    best_model = random_search.best_estimator_

    # Validation
    y_valid_pred = best_model.predict(X_valid_preprocessed)
    valid_f1 = f1_score(y_valid, y_valid_pred, average='macro')
    print(f'Validation F1 Score: {valid_f1}')
    print("\nClassification Report (Validation):\n", classification_report(y_valid, y_valid_pred))

    # Cross-validation
    cv_scores = cross_val_score(best_model, X_train_preprocessed, y_train, cv=StratifiedKFold(5), scoring='f1_macro')
    print(f'Cross-validation F1 Scores: {cv_scores}')
    print(f'Mean Cross-validation F1 Score: {np.mean(cv_scores)}')

    # Run predictions and save to file
    make_predictions(test_file, predictions_file, features, preprocessor, best_model)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--train-file", type=str, help="File path of train.csv")
    parser.add_argument("--test-file", type=str, help="File path of test.csv")
    parser.add_argument("--predictions-file", type=str, help="Save path of predictions")
    args = parser.parse_args()

    main(args.train_file, args.test_file, args.predictions_file)
